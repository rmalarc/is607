---
title: 'Finding Related Diseases by Scraping Pubmed'
author: "Mauricio Alarcon"
date: "April 23, 2015"
output: html_document
---

#Summary

When analyzing health data, it is very important to define control groups. The definition of control groups can be quite challenging and often times a limitation for large-scale data mining projects.

Let's say that we want to analyze a set of observations for patients with end stage kidney disease with the goal of establishing an early diagnosis. 

We may want to know what type of observations are more often found in this population. In order to do this analysis, we need a good control group, preferable composed of precursors or populations that may be at a high risk for developing the disease of interest.

The purpose of this project is to build a database of diseases and other related diseases by scraping a well-known clinical-literature search engine such as [PubMed](http://www.ncbi.nlm.nih.gov/pubmed).

The assumption is that if a publication contains references to conditions A and B, there may be a relationship between these two. If the above it is true for a large number of publications, it may suggest that the disease combination could be a good candidate for further analysis.

We will use disease terms found in ICD9 classification data.

The goal is to obtain frequency counts of:
* Individual disease terms: Diabetes, Cancer, HIV, etc.
* All disease terms combinations: Diabetes AND Cancer, Diabetes AND HIV, etc.


* * * 

#The Libraries

```{r, warning=FALSE, message=FALSE}
library(RCurl)
library(XML)
library(rvest)
library(knitr)
library(plyr)
library(stringr)
library(rmongodb)
library(rjson)
library(tm)
```

* * * 

#Loading Diesease Phrases

The Healthcare Utilization Project (HCUP) has an ICD9 classification data available in [their website](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp).

There is one file in particular that contains diagnosis category names. This list is great as the disease labels are in [plain english](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/CCSCategoryNames_FullLabels.pdf). 

A CSV version of this file is available in the [Single-Level Diagnosis CCS Categories](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/Single_Level_CCS_2015.zip). This is the data that we will use as our list of diseases.


```{r}
# get files diagnosis category label from CSV in github
dxref <- "https://raw.githubusercontent.com/rmalarc/is607/master/final_project/Single_Level_CCS_2015/dxlabel%202013.csv"
dxref_data_csv <- getURL(dxref)
dxref_data <- read.csv(text=dxref_data_csv,head=TRUE,sep=",",as.is=TRUE)

#clean up column names and turn to lower_case
colnames(dxref_data)<-tolower(gsub("\\.","_",colnames(dxref_data)))

kable(summary(dxref_data))

kable(head(dxref_data,n=25))
```

* * * 

#Preparing the Disease Terms

In order to use the disease terms we need to:

* Clean up strings (remove numbers, punctuation, convert to lower-case)
* Remove stop words
* URL Encode the phrases

```{r}

# flag those terms that appear to be invalid
dxref_data$is_valid <- TRUE
dxref_data[dxref_data$ccs_diagnosis_categories %in%
             c(".Z  ",".   ",".A  ","254","255","256","257","258","259"),"is_valid"] <-FALSE 
dxref_data[grep("E Codes: ",dxref_data$ccs_diagnosis_categories_labels),"is_valid"] <-FALSE 

# gsub helper function
toString <- content_transformer(function(x, from, to) gsub(from, to, x))

#load data into corpus
dxref_data_corpus <- Corpus(VectorSource(dxref_data[2]))

#remove numbers
dxref_data_corpus <- tm_map(dxref_data_corpus, removeNumbers)

# to lowercase
dxref_data_corpus <- tm_map(dxref_data_corpus, content_transformer(tolower))

# Hyphens to spaces
dxref_data_corpus <- tm_map(dxref_data_corpus, toString, "-", " ")

# remove the stuff in parentesis
dxref_data_corpus <- tm_map(dxref_data_corpus, toString, "\\(.+\\)", "")

# replace / or - for whitespace
dxref_data_corpus <- tm_map(dxref_data_corpus, toString, "(/|-)", " ")

#remove punctuation
dxref_data_corpus <- tm_map(dxref_data_corpus, removePunctuation)


# remove english stop words
dxref_data_corpus <- tm_map(dxref_data_corpus, removeWords, stopwords("english"))

# strip double white spaces
dxref_data_corpus <- tm_map(dxref_data_corpus, toString, "^\\s+|\\s+$", "")
dxref_data_corpus <- tm_map(dxref_data_corpus, toString, "  ", " ")
dxref_data_corpus <- tm_map(dxref_data_corpus, stripWhitespace)

# merge with dataframe
dxref_data$disease_phrase <- dxref_data_corpus[["1"]][["content"]]
dxref_data$disease_phrase <- gsub(" "," AND ",dxref_data$disease_phrase)
dxref_data$url_params <- sapply(dxref_data$disease_phrase,URLencode)

```

* * * 

#Helper Functions

In order to facilitate scraping more than one page, we will use the following functions:

```{r}
####################################################################################
# pubmed_entries_for(URLParams)
#
# This function scrapes pubmed by searching for the specified URL encoded keyword.
# The function returns the number of entries in pubmed for the keyword.
####################################################################################

pubmed_entries_for <-function(URLParams) {
  Sys.sleep(0.5)
  baseURL <- "http://www.ncbi.nlm.nih.gov/pubmed?term="
  theURL <- paste(baseURL,URLParams,sep="")
  page_data <- html(theURL)

  pubmed_results<- page_data %>% 
      html_nodes(xpath='//h2[contains(@class,"result_count")]')

  pubmed_results<- sapply(pubmed_results,xmlValue)

  if(length(pubmed_results) > 0){
    pubmed_results<-as.integer(str_extract(pubmed_results[1],"[0-9]+$"))
  } else {
    pubmed_results <- 0
  }

  return(pubmed_results)
}

####################################################################################
# scrape_pubmed_entries(ref_data, entry_type)
#
# This function accepts a dataframe containing the entries to process as well as an entry_type.
# The DF must contain a column called url_params, which are the URL parameters passed to
# pubmed_entries_for.
# 
# The processed output is saved to MongoDB and returned as a dataframe
####################################################################################
scrape_pubmed_entries <- function(ref_data,entry_type) {
  #ref_data<-dxref_data_x_join
  #entry_type<-"intersect_count"
  
  coll<-paste(ns,entry_type,sep=".")

  # get previously exported records
  entries_processed <- NULL
  cursor<-mongo.find(m
                     ,coll
                     ,query = mongo.bson.empty()
                     ,fields=list(url_params=1L)
                     )
  entries_processed <- mongo.cursor.to.data.frame(cursor)
  mongo.cursor.destroy(cursor)

  # process records not already processed
  entries_to_process <- ref_data[!(ref_data$url_params %in% entries_processed$url_params),]

  if(nrow(entries_to_process)>0){
    for (i in 1:nrow(entries_to_process)){
      entry<-entries_to_process[i,]
      # get the number of hits for entry in pubmed
      hits<-pubmed_entries_for(entry$url_params)

      # resulting DF
      results<-entry
      results$entry_type <- entry_type
      results$hits<-hits

      # export to mongo
      mongo.insert(m, coll,mongo.bson.from.JSON(toJSON(results)))
    }
  }
  
  # get all processed entries
  cursor <- mongo.find(m
                       ,coll
                       ,query = mongo.bson.empty()
                       )
  entries_processed <- mongo.cursor.to.data.frame(cursor)
  mongo.cursor.destroy(cursor)

  return(entries_processed)
}
```

* * * 

#Scraping the Pubmed Search Engine

##Getting Frequency Counts of Individual Disease Terms

```{r}

# connect to monogo so we can export the results of the scrape a row at a time
m <- mongo.create()
ns <- "scrape_pubmed"


# yes, it's a loop. The apply functions are faster. However, the function gets delayed by a sec
# each time. The loop allows me to restart where I left off
entries_processed <- NULL

# If you want to restart the whole process from scratch, set restart to TRUE
restart <- FALSE
if (restart){
  #delete all previously processed entries by purging the contents of the MongoDB
  mongo.remove(m, ns) 
}

absolute_count_entries_processed <- scrape_pubmed_entries(dxref_data,"absolute_count")


# show a sample of the output
kable(head(absolute_count_entries_processed,n=25))

```

* * * 

##Getting Frequency Counts of Disease Term Combinations (ie: tuberculosis AND hiv)

```{r}
# x-join all the VALID terms
dxref_data_x_join <- merge(x = absolute_count_entries_processed[absolute_count_entries_processed$is_valid&(absolute_count_entries_processed$hits>0),]
                           , y = absolute_count_entries_processed[absolute_count_entries_processed$is_valid&(absolute_count_entries_processed$hits>0),]
                           , by = NULL)

dxref_data_x_join$ccs_diagnosis_categories.x<-as.character(dxref_data_x_join$ccs_diagnosis_categories.x)
dxref_data_x_join$ccs_diagnosis_categories.y<-as.character(dxref_data_x_join$ccs_diagnosis_categories.y)

#Remove permutations, we just want combinations
dxref_data_x_join <- dxref_data_x_join[dxref_data_x_join$ccs_diagnosis_categories.x
                                       >dxref_data_x_join$ccs_diagnosis_categories.y,]

# generate combined URL parameters
dxref_data_x_join$url_params <- sprintf("(%s)AND(%s)",dxref_data_x_join$url_params.x,dxref_data_x_join$url_params.y)

#scrape pubmed
intersect_count_entries_processed <- scrape_pubmed_entries(dxref_data_x_join,"intersect_count")

#show a sample of the output
kable(head(intersect_count_entries_processed,n=25))

```

* * * 

#Individual Disease Terms Sorted by Absolute Frequency

```{r}
# Estimate the total number of UNIQUE documents
total_number_of_documents<-sum(absolute_count_entries_processed$hits)*0.5


absolute_count_entries_processed$p_hit <- absolute_count_entries_processed$hits/total_number_of_documents

kable(
  absolute_count_entries_processed[
    order(-absolute_count_entries_processed$p_hit)
    ,c("ccs_diagnosis_categories_labels"
       ,"hits"
       ,"p_hit"
      )
    ]
)

```

* * * 

#Top-500 Disease Term Combinations Sorted by Absolute Frequency

```{r}
summary<-intersect_count_entries_processed[c("ccs_diagnosis_categories_labels.x"
                                             ,"ccs_diagnosis_categories_labels.y"
                                             ,"url_params"
                                             ,"hits.x"
                                             ,"hits.y"
                                             ,"hits")]

summary$p_intersect_given_xy<-summary$hits/(summary$hits.x+summary$hits.y)


summary$p_intersect<-summary$hits/total_number_of_documents

kable(
  head(
    summary[
      order(-summary$p_intersect)
      ,c("ccs_diagnosis_categories_labels.x"
         ,"ccs_diagnosis_categories_labels.y"
         ,"hits.x"
         ,"hits.y"
         ,"hits"
         ,"p_intersect_given_xy"
         ,"p_intersect"
         )
      ]
    ,n=500
    )
  )

```
